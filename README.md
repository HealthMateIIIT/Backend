# ğŸ§  Disease Query Processing Server

## ğŸ“‹ Overview

This project implements a **Flask-based server** that takes **natural language health-related queries** (like â€œI have a headache and feverâ€ or â€œWhat are the precautions for dengue?â€) and returns **appropriate responses** such as predicted diseases, symptoms, or precautions.

The system acts as an **intelligent query-processing pipeline** where multiple internal models and datasets are coordinated by an **LLM (Google Gemini)** that decides:

* What kind of query the user has asked (symptom-based, disease-based, or precaution-based),
* Which internal model or function to invoke,
* How to interpret the modelâ€™s output,
* And finally, how to convert the raw output into a user-friendly natural language response.

---

## ğŸ§© Project Architecture

### ğŸ”„ Flow

```
User Query â†’ Flask API â†’ Gemini LLM â†’ Model Selector â†’ Internal Model(s) â†’ LLM Response Formatter â†’ Output to User
```

### âš™ï¸ Step-by-Step Flow Description

1. **User Input (Frontend or API Call)**
   The user sends a text query to the Flask endpoint (e.g., `/query`).

2. **Query Processing (LLM Decision Layer)**
   The query is sent to **Gemini API**, which analyzes and decides:

   * What the user is asking (symptoms â†’ disease, disease â†’ precautions, etc.),
   * Which model or function to use,
   * What kind of output is expected.

3. **Model Execution Layer**
   Based on Geminiâ€™s decision:

   * If the input is *symptoms*, a **Symptom-to-Disease model** is called.
   * If the input is *disease*, a **Disease-to-Precautions** or **Disease-to-Symptoms** model is used.

   These functions return mock/random outputs for now (to simulate model responses).

4. **Output Interpretation (LLM Again)**
   The LLM reformats the raw output into a **human-readable** explanation.

5. **Final Response (Flask)**
   The Flask server returns a JSON response back to the client.

---

## ğŸ“‚ Directory Structure

```
project-root/
â”‚
â”œâ”€â”€ server.py                     # Main Flask application
â”œâ”€â”€ requirements.txt           # Dependencies list
â”‚
â”œâ”€â”€ dataset/                   # Data folder
â”‚   â”œâ”€â”€ Disease_precaution.csv # Disease â†’ Precautions mapping
â”‚   â””â”€â”€ DiseaseAndSymptoms.csv # Disease â†’ Symptoms mapping
â”‚
â”œâ”€â”€ models/                    # Placeholder for model logic
â”‚   â”œâ”€â”€ disease_to_precaution.py
â”‚   â”œâ”€â”€ disease_to_symptom.py
â”‚   â””â”€â”€ symptom_to_disease.py
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ llm_handler.py         # Handles Gemini API calls and reasoning
â”‚
â””â”€â”€ README.md                  # This documentation
```

---

## ğŸ§  Role of Gemini LLM

Gemini acts as the **intelligent brain** of the system.

It performs three major tasks:

1. **Intent Recognition**
   Understands what the user query means (e.g., â€œI have cough and feverâ€ â†’ user is describing symptoms).

2. **Routing Decision**
   Determines which internal model or dataset function to invoke.

3. **Response Formatting**
   Takes raw model output (like probabilities or lists) and generates a natural, human-friendly response.

You can access your Gemini API key in the system using:

```bash
echo $GEMINI_API
```

---

## ğŸ§ª Example Query Scenarios

| User Query                             | Expected Task   | Model Used            | Output Example                                 |
| -------------------------------------- | --------------- | --------------------- | ---------------------------------------------- |
| â€œI have fever and coughâ€               | Predict disease | symptom_to_disease    | â€œYou may have Flu, COVID, or Dengue.â€          |
| â€œWhat are the precautions for dengue?â€ | Get precautions | disease_to_precaution | â€œUse mosquito nets, stay hydrated, rest well.â€ |
| â€œTell me symptoms of malariaâ€          | Get symptoms    | disease_to_symptom    | â€œHigh fever, chills, sweating, and fatigue.â€   |

---

## ğŸš€ API Endpoints

### `POST /query`

#### Description:

Takes user input query and returns an intelligent response.

#### Request:

```json
{
  "query": "I have headache and sore throat"
}
```

#### Response (Example):

```json
{
  "status": "success",
  "detected_task": "symptom_to_disease",
  "top_diseases": ["Common Cold", "Flu", "COVID-19"],
  "response": "Based on your symptoms, you might have Common Cold, Flu, or COVID-19. Please consult a doctor if symptoms persist."
}
```

---

## ğŸ§° Mock Implementations

For now, the model functions will **return random/predefined results** to simulate responses.
Later, you can replace these stubs with actual ML models.

Example:

```python
# models/symptom_to_disease.py
import random

def predict_disease(symptoms):
    diseases = ["Flu", "Common Cold", "Dengue", "Malaria", "COVID-19"]
    return random.sample(diseases, 3)
```

---

## ğŸ§ª Running the Server

### 1ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Set the Gemini API Key

```bash
export GEMINI_API="AIzaSyBo6CfSxrCxNqXmkqSvo8ignurOA0vitwQ"
```

### 3ï¸âƒ£ Run Flask App

```bash
python server.py
```

### 4ï¸âƒ£ Test using cURL or Postman

```bash
curl -X POST http://127.0.0.1:5000/query -H "Content-Type: application/json" -d '{"query": "I have cough and fever"}'
```

---

## ğŸ“˜ Future Work

| Area                      | Description                                                        |
| ------------------------- | ------------------------------------------------------------------ |
| ğŸ” Intent Classification  | Use Gemini to detect query type more accurately                    |
| ğŸ¤– Model Integration      | Replace mock functions with trained models                         |
| ğŸ§¾ Response Summarization | Add LLM-based answer refinement                                    |
| ğŸ’¬ Frontend UI            | Add a chat interface where users can talk or speak (voice-to-text) |
| ğŸ§  Context Memory         | Allow multi-turn conversations with contextual awareness           |

---